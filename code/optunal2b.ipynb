{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial-setup",
   "metadata": {},
   "source": [
    "# Spatial Transcriptomics Analysis with Transformer-GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment-setup",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "environment-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System configuration\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "package-imports",
   "metadata": {},
   "source": [
    "## 2. Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 10:00:09.903553: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746284409.947798  350800 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746284409.960659  350800 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746284410.057993  350800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746284410.058008  350800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746284410.058009  350800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746284410.058010  350800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "/home/frankfurt/tf-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scanpy as sc\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.sparse \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "data-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path, n_top_genes=2000, target_cells=1_500_000):\n",
    "    # 1. Load data IN-MEMORY (remove 'backed' mode)\n",
    "    adata = sc.read_h5ad(file_path)\n",
    "    \n",
    "    # 2. Filter mitochondrial genes\n",
    "    adata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n",
    "    sc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\"], percent_top=None, inplace=True)\n",
    "    adata = adata[adata.obs[\"pct_counts_mt\"] < 20, :].copy()  # Force in-memory\n",
    "    \n",
    "    # 3. Subsample to target_cells (1.5M)\n",
    "    if adata.n_obs > target_cells:\n",
    "        sc.pp.subsample(adata, n_obs=target_cells, random_state=42, copy=False)\n",
    "    \n",
    "    # 4. Remove rare clusters\n",
    "    cluster_counts = adata.obs['cluster'].value_counts()\n",
    "    valid_clusters = cluster_counts[cluster_counts >= 2].index\n",
    "    adata = adata[adata.obs['cluster'].isin(valid_clusters)].copy()\n",
    "    \n",
    "    # 5. Ensure CSR sparse format\n",
    "    if not isinstance(adata.X, scipy.sparse.csr_matrix):\n",
    "        adata.X = scipy.sparse.csr_matrix(adata.X)\n",
    "    \n",
    "    # 6. Proceed with HVG selection and split\n",
    "    sc.pp.filter_genes(adata, min_counts=1)\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor=\"seurat_v3\")\n",
    "    adata = adata[:, adata.var[\"highly_variable\"]]\n",
    "\n",
    "    # Convert sparse matrix to dense array\n",
    "    X_dense = adata.X.toarray() if isinstance(adata.X, scipy.sparse.spmatrix) else adata.X\n",
    "\n",
    "    return train_test_split(\n",
    "        X_dense,  # Now returns dense array\n",
    "        adata.obs['cluster'].cat.codes.values,\n",
    "        test_size=0.15,\n",
    "        stratify=adata.obs['cluster'].cat.codes.values,\n",
    "        random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-architecture",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a5f160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.dense = layers.Dense(d_model, activation='relu')\n",
    "        \n",
    "    def call(self, coords):\n",
    "        # Simple positional encoding using coordinate projection\n",
    "        pos_emb = self.dense(coords)\n",
    "        return self.dropout(pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerGNNBlock(layers.Layer):\n",
    "    def __init__(self, units, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=units // num_heads  # Ensure divisibility\n",
    "        )\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(units*4, activation='gelu'),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(units)  # Match input dimension\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Attention operation\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output), attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-components",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialTransformerGNN(tf.keras.Model):\n",
    "    def __init__(self, input_dim, num_classes, units, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.feature_dim = input_dim - 2  # Last 2 columns are coordinates\n",
    "        \n",
    "        # Feature projection to (units - pos_encoding_dim)\n",
    "        self.feature_proj = tf.keras.Sequential([\n",
    "            layers.Dense(units - 128, activation='relu'),  # 128 = pos encoding dim\n",
    "            layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        \n",
    "        # Positional encoding fixed at 128 dimensions\n",
    "        self.pos_encoder = PositionalEncoding(128, dropout_rate)\n",
    "        \n",
    "        # Transformer blocks now match units dimension\n",
    "        self.transformer_block1 = TransformerGNNBlock(units, num_heads, dropout_rate)\n",
    "        self.transformer_block2 = TransformerGNNBlock(units, num_heads, dropout_rate)\n",
    "        \n",
    "        # Output layers\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.classifier = layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Split inputs\n",
    "        features = inputs[:, :self.feature_dim]\n",
    "        coords = inputs[:, self.feature_dim:]\n",
    "        \n",
    "        # Process features with explicit sequence dimension\n",
    "        x = self.feature_proj(features)\n",
    "        pos_emb = self.pos_encoder(coords)\n",
    "        x = tf.concat([x, tf.expand_dims(pos_emb, 1)], axis=-1)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        x, _ = self.transformer_block1(x)\n",
    "        x, _ = self.transformer_block2(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.flatten(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-setup",
   "metadata": {},
   "source": [
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_training(model, learning_rate):\n",
    "    \"\"\"Configure training parameters and callbacks\"\"\"\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return [\n",
    "        EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyperparameter-tuning",
   "metadata": {},
   "source": [
    "## 6. Optuna Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "optuna-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optuna_study():\n",
    "    return optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "    )\n",
    "\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "    # Model architecture parameters\n",
    "    model_params = {\n",
    "        'input_dim': X_train.shape[1],\n",
    "        'num_classes': len(np.unique(y_train)),\n",
    "        'units': trial.suggest_int('units', 128, 512, step=64),\n",
    "        'num_heads': trial.suggest_int('num_heads', 2, 8),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
    "    }\n",
    "    \n",
    "    # Training parameters (separate from model params)\n",
    "    training_params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [128, 256, 512])\n",
    "    }\n",
    "\n",
    "    # Initialize model with only architecture parameters\n",
    "    model = SpatialTransformerGNN(**model_params)\n",
    "    \n",
    "    # Use training_params for training configuration\n",
    "    callbacks = configure_training(model, training_params['learning_rate'])\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=training_params['batch_size'],  # <-- Use here\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return max(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization",
   "metadata": {},
   "source": [
    "## 7. Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "viz-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execution",
   "metadata": {},
   "source": [
    "## 8. Main Execution Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-flow",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    (X_train, X_val, y_train, y_val) = load_and_preprocess_data(\n",
    "        '/home/frankfurt/LDL/data/abc_atlas/Zhuang-ABCA-1-merged-final-CLEAN-v2.h5ad',\n",
    "        n_top_genes=2000,\n",
    "        target_cells=1_500_000\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter optimization\n",
    "        # Create study with data closure\n",
    "    study = create_optuna_study()\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_train, y_train, X_val, y_val),  # Data closure\n",
    "        n_trials=50,\n",
    "        timeout=3600\n",
    "    )\n",
    "    \n",
    "    # Train final model\n",
    "best_params = study.best_params\n",
    "\n",
    "# Separate model params from training params\n",
    "model_params = {\n",
    "    'input_dim': X_train.shape[1],\n",
    "    'num_classes': len(np.unique(y_train)),\n",
    "    'units': best_params['units'],\n",
    "    'num_heads': best_params['num_heads'],\n",
    "    'dropout_rate': best_params['dropout_rate']\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "    'learning_rate': best_params['learning_rate'],\n",
    "    'batch_size': best_params['batch_size']\n",
    "}\n",
    "\n",
    "# Initialize final model\n",
    "final_model = SpatialTransformerGNN(**model_params)\n",
    "\n",
    "# Train with separated training params\n",
    "history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=training_params['batch_size'],\n",
    "    callbacks=configure_training(final_model, training_params['learning_rate'])\n",
    ")\n",
    "    \n",
    "    # Evaluation and visualization\n",
    "plot_training_history(history)\n",
    "final_model.evaluate(X_val, y_val, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
